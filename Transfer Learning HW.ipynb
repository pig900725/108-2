{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 2.x版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.載入套件、資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Layers for FNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# Layers for CNN\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# For data preprocessing\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 626s 4us/step\n"
     ]
    }
   ],
   "source": [
    "#用CIFAR10資料集\n",
    "name_list = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "(x_train, y_train0), (x_test, y_test0) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the range of featurs\n",
    "x_train = x_train / x_train.max()\n",
    "x_test = x_test / x_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "y_train = to_categorical(y_train0, 10)\n",
    "y_test = to_categorical(y_test0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.random.randint(x_train.shape[0])\n",
    "x_sample = x_train[idx]\n",
    "y_sample = y_train0[idx].squeeze()\n",
    "\n",
    "plt.imshow(x_sample)\n",
    "plt.title(name_list[y_sample])\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷積神經網路層\n",
    "CNN_layers = [Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding='same', activation='relu', name='Conv_1'),\n",
    "              MaxPool2D(),\n",
    "              Conv2D(160, (3, 3), padding='same', activation='relu', name='Conv_2'),\n",
    "              MaxPool2D(),\n",
    "              Conv2D(800, (3, 3), padding='same', activation='relu', name='Conv_3'),\n",
    "              GlobalAveragePooling2D()]\n",
    "# 全連結層\n",
    "FC_layers = [Dense(units=160, activation='relu'),\n",
    "             Dense(units=10, activation='softmax')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x2120871e848>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x2120871ee08>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x21208718308>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x21208718888>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x21208718d48>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x21208719408>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x2120871e808>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x21208719e48>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_layers + FC_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv_1 (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv2D)              (None, 16, 16, 160)       46240     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 160)         0         \n",
      "_________________________________________________________________\n",
      "Conv_3 (Conv2D)              (None, 8, 8, 800)         1152800   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 160)               128160    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1610      \n",
      "=================================================================\n",
      "Total params: 1,329,706\n",
      "Trainable params: 1,329,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(CNN_layers+FC_layers)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "                optimizer=Adam(),\n",
    "                metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 382s 8ms/sample - loss: 1.7983 - categorical_accuracy: 0.3166 - val_loss: 1.5710 - val_categorical_accuracy: 0.4167\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 364s 7ms/sample - loss: 1.4568 - categorical_accuracy: 0.4627 - val_loss: 1.3553 - val_categorical_accuracy: 0.4991\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 321s 6ms/sample - loss: 1.2856 - categorical_accuracy: 0.5320 - val_loss: 1.2070 - val_categorical_accuracy: 0.5621\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 273s 5ms/sample - loss: 1.1740 - categorical_accuracy: 0.5765 - val_loss: 1.1346 - val_categorical_accuracy: 0.5890\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 272s 5ms/sample - loss: 1.0788 - categorical_accuracy: 0.6128 - val_loss: 1.0572 - val_categorical_accuracy: 0.6196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2120ac22188>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          batch_size=128, \n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存下來\n",
    "model.save_weights('LeNet5_CIFAR10_HW.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 101s 2ms/sample - loss: 1.0309 - categorical_accuracy: 0.6327\n",
      "10000/10000 [==============================] - 24s 2ms/sample - loss: 1.0572 - categorical_accuracy: 0.6196\n",
      "Train Accuracy: 63.26799988746643\n",
      "Test Accuracy: 61.959999799728394\n"
     ]
    }
   ],
   "source": [
    "score_train = model.evaluate(x_train, y_train)\n",
    "score_test = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Train Accuracy: {score_train[1]*100}')\n",
    "print(f'Test Accuracy: {score_test[1]*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遷移學習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀入fashion_mnist 資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "(x_train2, y_train2), (x_test2, y_test2) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = x_train2.reshape(60000, 28, 28, 1)/255\n",
    "x_test2 = x_test2.reshape(10000, 28, 28, 1)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2 = to_categorical(y_train2, 10)\n",
    "y_test2 = to_categorical(y_test2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#開一個函數學習機\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D \n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD #學習方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_layer2 = [Conv2D(32, (3, 3), input_shape=(28, 28, 1), padding='same', activation='relu', name='Conv_1'),\n",
    "              MaxPool2D(),\n",
    "              Conv2D(64, (3, 3), padding='same', activation='relu', name='Conv_2'),\n",
    "              MaxPool2D(),\n",
    "              Conv2D(128, (3, 3), padding='same', activation='relu', name='Conv_3'),\n",
    "              GlobalAveragePooling2D()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_layer2 = [Dense(units=256, activation='relu'),\n",
    "             Dense(units=10, activation='softmax')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv_1 (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv2D)              (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "Conv_3 (Conv2D)              (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 128,266\n",
      "Trainable params: 128,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential(CNN_layer2+FC_layer2)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', \n",
    "               optimizer=Adam(), \n",
    "               metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch_size=256 /epochs=10來訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 1.0707 - categorical_accuracy: 0.6053 - val_loss: 0.7514 - val_categorical_accuracy: 0.7156\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 59s 979us/sample - loss: 0.6865 - categorical_accuracy: 0.7431 - val_loss: 0.6459 - val_categorical_accuracy: 0.7509\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 59s 985us/sample - loss: 0.6106 - categorical_accuracy: 0.7717 - val_loss: 0.6317 - val_categorical_accuracy: 0.7680\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 60s 1000us/sample - loss: 0.5558 - categorical_accuracy: 0.7965 - val_loss: 0.5534 - val_categorical_accuracy: 0.7924\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.5102 - categorical_accuracy: 0.8157 - val_loss: 0.5130 - val_categorical_accuracy: 0.8169\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 60s 994us/sample - loss: 0.4751 - categorical_accuracy: 0.8303 - val_loss: 0.4949 - val_categorical_accuracy: 0.8248\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.4505 - categorical_accuracy: 0.8390 - val_loss: 0.4850 - val_categorical_accuracy: 0.8265\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.4229 - categorical_accuracy: 0.8490 - val_loss: 0.4423 - val_categorical_accuracy: 0.8399\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.4088 - categorical_accuracy: 0.8515 - val_loss: 0.4259 - val_categorical_accuracy: 0.8497\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.3868 - categorical_accuracy: 0.8605 - val_loss: 0.3950 - val_categorical_accuracy: 0.8602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x212115a2cc8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train2, y_train2,\n",
    "           batch_size=256,\n",
    "           epochs=10, \n",
    "           validation_data=(x_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 20s 327us/sample - loss: 0.3623 - categorical_accuracy: 0.8682\n",
      "10000/10000 [==============================] - 6s 553us/sample - loss: 0.3950 - categorical_accuracy: 0.8602\n",
      "Train Accuracy: 86.8233323097229\n",
      "Test Accuracy: 86.01999878883362\n"
     ]
    }
   ],
   "source": [
    "score_train = model2.evaluate(x_train2, y_train2)\n",
    "score_test = model2.evaluate(x_test2, y_test2)\n",
    "\n",
    "print(f'Train Accuracy: {score_train[1]*100}')\n",
    "print(f'Test Accuracy: {score_test[1]*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
